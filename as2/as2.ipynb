{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Language Modelling in Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Student Name: Kexin Chen\n",
    "\n",
    "Student ID: 1240143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Due date</b>: Wednesday, 3 April 2024 1pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day (both week and weekend days counted)\n",
    "\n",
    "<b>Marks</b>: 8% of mark for class (with 7% on correctness + 1% on quality and efficiency of your code)\n",
    "\n",
    "<b>Materials</b>: See \"Using Jupyter Notebook and Python page\" on Canvas (under Modules>Resources) for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, Gensim, Keras and Pytorch. We recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages (the packages listed above are all fine to use); if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3.8</b>; libraries/packages should be in their latest version that is compatible with python3.8.  \n",
    "\n",
    "To familiarize yourself with NLTK, here is a free online book:  Steven Bird, Ewan Klein, and Edward Loper (2009). <a href=https://www.nltk.org/book/>Natural Language Processing with Python</a>. O'Reilly Media Inc. You may also consult the <a href=https://www.nltk.org/api/nltk.html>NLTK API</a>.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each question is worth is explicitly given. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board; we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place. In regards to the use of artificial intelligence tools in the context of academic integrity, please see the university's statement <a href=\"https://academicintegrity.unimelb.edu.au/plagiarism-and-collusion/artificial-intelligence-tools-and-technologies\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this homework, you'll be creating an 'artificial intelligence' player for the classic Hangman word guessing game. You will need to implement several different automatic strategies based on character-level n-gram language models. Your objective is to create an automatic player which makes the fewest mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hangman Game (7 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: The <a href=\"https://en.wikipedia.org/wiki/Hangman_(game)\">Hangman game</a> is a simple game whereby one person thinks of a word, which they keep secret from their opponent, who tries to guess the word one character at a time. The game ends when the opponent makes more than a fixed number of incorrect guesses, or they figure out the secret word before then (in which case they *win*). \n",
    "\n",
    "Here's a simple version of the game. **No implementation is needed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hangman(secret_word, guesser, max_mistakes=8, verbose=True, **guesser_args):\n",
    "    \"\"\"\n",
    "        This function plays the hangman game with the provided guesser and returns the number of incorrect guesses. \n",
    "        \n",
    "        secret_word: a string of lower-case alphabetic characters, i.e., the answer to the game\n",
    "        guesser: a function which guesses the next character at each stage in the game\n",
    "            The function takes a:\n",
    "                mask: what is known of the word, as a string with _ denoting an unknown character\n",
    "                guessed: the set of characters which already been guessed in the game\n",
    "                guesser_args: additional (optional) keyword arguments, i.e., name=value\n",
    "        max_mistakes: limit on length of game, in terms of number of allowed mistakes\n",
    "        verbose: silent or verbose diagnostic prints\n",
    "        guesser_args: keyword arguments to pass directly to the guesser function\n",
    "    \"\"\"\n",
    "    secret_word = secret_word.lower()\n",
    "    mask = ['_'] * len(secret_word)\n",
    "    guessed = set()\n",
    "    if verbose:\n",
    "        print(\"Starting hangman game. Target is\", ' '.join(mask), 'length', len(secret_word))\n",
    "    \n",
    "    mistakes = 0\n",
    "    while mistakes < max_mistakes:\n",
    "        if verbose:\n",
    "            print(\"You have\", (max_mistakes-mistakes), \"attempts remaining.\")\n",
    "        guess = guesser(mask, guessed, **guesser_args)\n",
    "\n",
    "        if verbose:\n",
    "            print('Guess is', guess)\n",
    "        if guess in guessed:\n",
    "            if verbose:\n",
    "                print('Already guessed this before.')\n",
    "            mistakes += 1\n",
    "        else:\n",
    "            guessed.add(guess)\n",
    "            if guess in secret_word and len(guess) == 1:\n",
    "                for i, c in enumerate(secret_word):\n",
    "                    if c == guess:\n",
    "                        mask[i] = c\n",
    "                if verbose:\n",
    "                    print('Good guess:', ' '.join(mask))\n",
    "            else:\n",
    "                if len(guess) != 1:\n",
    "                    print('Please guess with only 1 character.')\n",
    "                if verbose:\n",
    "                    print('Sorry, try again.')\n",
    "                mistakes += 1\n",
    "                \n",
    "        if '_' not in mask:\n",
    "            if verbose:\n",
    "                print('Congratulations, you won.')\n",
    "            return mistakes\n",
    "        \n",
    "    if verbose:\n",
    "        print('Out of guesses. The word was', secret_word)    \n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a human guesser allowing interactive play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    This is a simple function for manual play.\n",
    "    \"\"\"\n",
    "    print('\\nEnter your guess:')\n",
    "    return input().lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to play hangman interactively, please set `interactive` to `True`. When submitting your solution, set to `False` so we can automatically run the whole notebook using `Run All`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>\n",
    "\n",
    "You can play the game interactively using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    hangman('whatever', human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Question 1 (1.0 mark)\n",
    "\n",
    "**Instructions**: We will use the words in NLTK's Brown corpus for training an artificial intelligence guessing algorithm, and for evaluating the quality of the algorithm.\n",
    "\n",
    "Your first task is to compute the number of **unique word types** occurring in the Brown corpus, using `nltk.corpus.brown` and the `words` method, and select only words that are **entirely comprised of alphabetic characters**. You should also **lowercase the words**. Finally, randomly shuffle (`numpy.random.shuffle`) this collection of word types, and split them into disjoint training and testing sets. The test set should contain 1000 word types, and the rest should be in the  training set. Note that we are intentionally making the hangman game hard, as the AI will need to cope with test words that it has not seen before, hence it will need to learn generalisable patterns of characters to make reasonable predictions.\n",
    "\n",
    "**Task**: Collect all unique word types from the Brown corpus, and produce `training_set` and `test_set`, 2 lists that contain 2 disjointed sets of words. Both `training_set` and `test_set` should be a python `list` (as initialised in the code). `test_set` must contain exactly 1000 word types.\n",
    "\n",
    "**Check**: Use the assertion statements in <b>\"For your testing\"</b> below for the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/clarec/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word types in test = 1000\n",
      "Number of word types in train = 39234\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('brown')\n",
    "np.random.seed(1)\n",
    "\n",
    "# training_set stores the rest word types for training\n",
    "training_set = []\n",
    "# test_set stores 1000 word types for testing\n",
    "test_set = []\n",
    "\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "unique_words = list(set([x.lower() for x in brown.words() if x.isalpha()]))\n",
    "np.random.shuffle(unique_words)\n",
    "\n",
    "test_set = unique_words[:1000]\n",
    "training_set = unique_words[1000:]\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(\"Number of word types in test =\", len(test_set))\n",
    "print(\"Number of word types in train =\", len(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(training_set) > 35000 and len(training_set) < 45000)\n",
    "assert(len(test_set) == 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Play the game**:\n",
    "\n",
    "Let's see how good you are at this game! Try to guess a random word from the test set. It is surprisingly difficult (and addictive)! Don't forget to set `interactive = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play hangman using random words from test set\n",
    "if interactive:\n",
    "    hangman(np.random.choice(test_set), human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (1.0 mark)\n",
    "\n",
    "**Instructions**: To set a baseline, your first AI attempt will be a trivial **random method**. For this you should implement a guessing method, similar to the `human` method above, i.e., using the same input arguments and returning a character. Your method should randomly choose a character from the range `a ... z` after excluding the characters that have already been guessed in the current game (all subsequent AI approaches should also exclude previous guesses).\n",
    "\n",
    "To help you measure the performance of this (and later) guesser, a `test_guesser` method that takes a guesser and measures the average number of incorrect guesses made over all the words in the `test` is provided to you. \n",
    "\n",
    "**Task**: Complete the `random_guesser` method. It should return a random character from the English alphabets.\n",
    "\n",
    "**Check**: Use the assertion statements in <b>\"For your testing\"</b> below for the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_guesser(guesser, test):\n",
    "    \"\"\"\n",
    "        This function takes a guesser and measures the average number of incorrect guesses made over all the words in the test_set. \n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for word in test:\n",
    "        total += hangman(word, guesser, 26, False)\n",
    "    return total / float(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing word = prettyman\n",
      "Number of mistakes made by the random guesser = 18\n",
      "\n",
      "Testing the random guesser using every word in test set\n",
      "Average number of incorrect guesses:  16.737\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def random_guesser(mask, guessed, **kwargs):\n",
    "    \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    lower = set(string.ascii_lowercase)  \n",
    "    unguessed = list(lower - guessed)\n",
    "    \n",
    "    return np.random.choice(unguessed)\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "random_word = np.random.choice(test_set)\n",
    "print(\"Guessing word =\", random_word)\n",
    "print(\"Number of mistakes made by the random guesser =\", hangman(random_word, random_guesser, 26, False))\n",
    "\n",
    "result = test_guesser(random_guesser, test_set)\n",
    "print(\"\\nTesting the random guesser using every word in test set\")\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result > 10 and result < 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (1.0 mark)\n",
    "\n",
    "**Instructions:** As your first real AI, you should train a **unigram language model** over the training set. This requires you to find the frequencies of characters over all training words. Using this model, you should write a guesser that returns the character with the highest probability. Remember to exclude already guessed characters. \n",
    "\n",
    "**Task**: Collect the frequencies of characters and store them in `unigram_counts` (use the first answer space). Complete the `unigram_guesser` method (use the second answer space). Note that it takes `unigram_counts` as an additional argument.\n",
    "\n",
    "**Check**: Use the assertion statements in <b>\"For your testing\"</b> below for the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the unigram guesser using every word in test set\n",
      "Average number of incorrect guesses:  10.133\n"
     ]
    }
   ],
   "source": [
    "unigram_counts = None\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "from collections import defaultdict\n",
    "\n",
    "unigram_counts = defaultdict(int)\n",
    "for word in training_set:\n",
    "    for c in word:\n",
    "        unigram_counts[c] += 1\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "def unigram_guesser(mask, guessed, unigram_counts=unigram_counts):\n",
    "\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \n",
    "    # Find the un-guessed character with the highest frequency\n",
    "    \n",
    "    sorted_counts = {k: v for k, v in sorted(unigram_counts.items(), key=lambda item: item[1])}\n",
    "    best_guess = None\n",
    "    for character, freq in sorted_counts.items():\n",
    "        if character not in guessed:\n",
    "            best_guess = character\n",
    "    \n",
    "    return best_guess\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "result = test_guesser(unigram_guesser, test_set)\n",
    "print(\"Testing the unigram guesser using every word in test set\")\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result > 5 and result < 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (1.0 mark)\n",
    "\n",
    "**Instructions:** The length of the secret word is an important clue that we might exploit. Different lengths tend to have different distributions over characters, e.g., short words are less likely to have suffixes or prefixes. You should incorporate this idea by conditioning the unigram model on the length of the secret word, i.e.,  having a **different unigram model for each length**. You will need to be a little careful at test time, to be robust to the situation that you encounter a word length that you didn't see in training. In such a case, your method should behave like the previous `unigram_guesser` in Question 3 (i.e., it guesses characters based on unigram frequencies, unconditioned by the word length).\n",
    "\n",
    "**Task**: Collect the frequencies of characters conditioned on the word length and store them in `unigram_counts_by_length` (use the first answer space). Complete the `unigram_length_guesser` method (use the second answer space).\n",
    "\n",
    "**Check**: Use the assertion statements in <b>\"For your testing\"</b> below for the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the length-conditioned unigram guesser using every word in test set\n",
      "Average number of incorrect guesses:  10.159\n"
     ]
    }
   ],
   "source": [
    "unigram_counts_by_length = None\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "from collections import defaultdict\n",
    "\n",
    "unigram_counts_by_length = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for word in training_set:\n",
    "    word_length = len(word)\n",
    "    for character in word:\n",
    "        unigram_counts_by_length[word_length][character] += 1\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "def unigram_length_guesser(mask, guessed, unigram_counts_by_length=unigram_counts_by_length, unigram_counts=unigram_counts):\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    word_length = len(mask)\n",
    "    best_guess = None\n",
    "    highest_freq = -1\n",
    "    if unigram_counts_by_length[word_length] != dict() and \\\n",
    "        any(c not in guessed for c in unigram_counts_by_length[word_length]):\n",
    "        for character, freq in unigram_counts_by_length[word_length].items():\n",
    "            if character not in guessed and freq > highest_freq:\n",
    "                best_guess = character\n",
    "                highest_freq = freq\n",
    "    else:\n",
    "        best_guess = unigram_guesser(mask, guessed, unigram_counts=unigram_counts)\n",
    "                \n",
    "    return best_guess\n",
    "\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "result = test_guesser(unigram_length_guesser, test_set)\n",
    "print(\"Testing the length-conditioned unigram guesser using every word in test set\")\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result > 5 and result < 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (1.0 mark)\n",
    "\n",
    "**Instructions:** Now for the next challenge, you'll build a **bigram language model** over characters. The order of characters is obviously important, yet this wasn't incorporated in any of the above models. Knowing that the word has the sequence `n _ s s` is a pretty strong clue that the missing character might be `e`. Similarly the distribution over characters that start or end a word are highly biased (e.g., toward common prefixes and suffixes, like *un-*, *-ed* and *-ly*).\n",
    "\n",
    "The task here is to develop a bigram language model over characters, and train it over the training words. Remember to be careful when handling the start of each word properly, e.g., by padding with a special starting symbol such as `$`. Do we also need a special ending symbol? That's for you to decide.\n",
    "\n",
    "Your bigram guesser should apply your language model to each blank position in the secret word by using its left context character. For example, in the partial word `e _ c _ b _ _` we know the left context for the first three blanks, but have no known left context for the last blank. In the case for the last blank, you should revert to using a unigram language model (since there's no context for us to use the bigram model). You should sum up the probability distribution (over all alphabets from <i>a</i> to <i>z</i>) for the 4 blanks, and select the alphabet with the highest probability that hasn't been guessed.\n",
    "\n",
    "**Note**:\n",
    "- When backing-off to the unigram language model, you **must use the vanilla unigram language model that you have built in Q3**. Do not use the length-based unigram language model, or any fancy variant, or you will lose marks.\n",
    "- You should build a **standard bigram language model**; i.e. do not do anything complicated like a bidirectional bigram language model.\n",
    "\n",
    "**Task**: Collect frequency counts that are necessary for building a bigram language model and store them in bigram_counts; feel free to add new objects if needed (use the first answer space). Complete the `bigram_guesser` method (use the second answer space). Note that the method currently only has one additional argument (`bigram_counts`), but you are free to add additional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the bigram guesser using every word in test set\n",
      "Average number of incorrect guesses:  8.838\n"
     ]
    }
   ],
   "source": [
    "bigram_counts = None\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for word in training_set:\n",
    "    padded_word = \"$\" + word  \n",
    "    for i in range(len(padded_word)):\n",
    "        if i < len(word):  \n",
    "            bigram_counts[padded_word[i]][word[i]] += 1\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "    \n",
    "\n",
    "def bigram_guesser(mask, guessed, bigram_counts=bigram_counts): # add extra arguments if needed\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    probabilities = defaultdict(int)\n",
    "\n",
    "    for i, char in enumerate(mask):\n",
    "        if char == '_':\n",
    "            left_context = mask[i-1] if i > 0 else '$'\n",
    "            for alphabet in 'abcdefghijklmnopqrstuvwxyz':\n",
    "                if alphabet not in guessed:\n",
    "                    if left_context in bigram_counts and alphabet in bigram_counts[left_context]:\n",
    "                        probabilities[alphabet] += bigram_counts[left_context][alphabet]\n",
    "                    else:\n",
    "                        probabilities[alphabet] += unigram_counts[alphabet]\n",
    "\n",
    "    # print(probabilities)\n",
    "    if not probabilities:\n",
    "        # Fallback to random guess from unguessed letters\n",
    "        unguessed_letters = [l for l in 'abcdefghijklmnopqrstuvwxyz' if l not in guessed_letters]\n",
    "        return random.choice(unguessed_letters)\n",
    "    best_guess = max(probabilities, key=probabilities.get)\n",
    "    return best_guess\n",
    "\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "result = test_guesser(bigram_guesser, test_set)\n",
    "print(\"Testing the bigram guesser using every word in test set\")\n",
    "print(\"Average number of incorrect guesses: \", result)\n",
    "\n",
    "# bigram_prediction = bigram_guesser(\"_pp_e\", guessed=set([\"p\", \"e\"]))\n",
    "# print(f\"The next letter to guess (bi-gram) is: {bigram_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 (1.5 mark)\n",
    "\n",
    "**Instructions:** You should try to develop a more effective AI for hangman. Feel free to engage your creativity here! Possibilities include better conditioning on the length of the word, fancier smoothing methods, ngram models and bidirectional models (lecture 8). Have fun!\n",
    "\n",
    "You will be marked based on the performance of your AI model, using a pre-made training and test set (created using a secret seed). Let x be the average number of mistakes in the test set, you will score:\n",
    "* 1.5 mark if x < 7.6\n",
    "* 1.0 mark if 7.6 <= x < 8.0\n",
    "* 0.5 mark if 8.0 <= x < 8.5\n",
    "* 0.0 mark if x >= 8.5\n",
    "\n",
    "Note:\n",
    "* When testing your AI model's performance, you may want to consider trying different training/test splits (using different seeds) to have a better understanding of its _average_ performance, as there will be some variance to its performance depending on the training/test split.\n",
    "* Your code must run under 2 minutes on Codalab; program that runs longer than that will be terminated and you will score 0.\n",
    "\n",
    "**Task** Complete the `my_amazing_ai_guesser` method, which implements a better language model for hangman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing my amazing AI guesser using every word in test set\n",
      "Average number of incorrect guesses:  7.685323953713616\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "k = 1\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        # Use a nested defaultdict to store frequencies by word length\n",
    "        self.freq = defaultdict(lambda: defaultdict(int))\n",
    "        # Probabilities also need to be stored by word length\n",
    "        self.prob = defaultdict(dict)\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "    \n",
    "    def insert(self, word):\n",
    "        word_length = len(word) - 1\n",
    "        for start in range(len(word)):\n",
    "            node = self.root\n",
    "            for char in word[start:]:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = TrieNode()\n",
    "                node.freq[word_length][char] += 1\n",
    "                node = node.children[char]\n",
    "            \n",
    "\n",
    "    def find_node(self, prefix):\n",
    "        node = self.root\n",
    "        for i in range(len(prefix)): \n",
    "            char = prefix[i]\n",
    "            if char in node.children:\n",
    "                # print(\"<query>\", char, \": \", node.freq)\n",
    "                node = node.children[char]\n",
    "            else:\n",
    "                return {}  # Prefix not found\n",
    "        return node\n",
    "        \n",
    "    def find_one_letter_freq(self, prefix, word_length):\n",
    "        node = self.find_node(prefix)\n",
    "        if node:\n",
    "            return self.find_node(prefix).freq[word_length]\n",
    "        return {}\n",
    "    \n",
    "    def get_most_probable_letter(self, prefix, word_length):\n",
    "        freq = self.find_one_letter_freq(prefix, word_length)   \n",
    "        # print(freq) \n",
    "        sorted_freq = sorted(freq, key = freq.get, reverse=True)\n",
    "        # print(sorted_freq)\n",
    "        if len(sorted_freq) > 0:\n",
    "            return sorted_freq[0]\n",
    "        return \"\"\n",
    "    \n",
    "    def calculate_probabilities(self):\n",
    "        def calculate_node_probabilities(node):\n",
    "            for word_length in node.freq:\n",
    "                total_freq = sum(node.freq[word_length].values()) + k * 26  # Smoothing applied\n",
    "                for char in node.freq[word_length]:\n",
    "                    # Calculate smoothed probability\n",
    "                    node.prob[word_length][char] = (node.freq[word_length][char] + k) / total_freq\n",
    "            for child in node.children.values():\n",
    "                calculate_node_probabilities(child)\n",
    "        calculate_node_probabilities(self.root)\n",
    "\n",
    "        \n",
    "    def get_next_char_probability(self, prefix, next_char, word_length):\n",
    "        node = self.root\n",
    "        for char in prefix:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "            else:\n",
    "                return k / (k * 26)\n",
    "        return node.prob[word_length].get(next_char, k / (k * 26))\n",
    "\n",
    "    def get_probability(self, prefix, word_length):\n",
    "        node = self.root\n",
    "        for char in prefix:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "            else:\n",
    "                return {}\n",
    "        return node.prob[word_length]\n",
    "\n",
    "def calculate_unigram_probabilities(words):\n",
    "    \"\"\"Calculate unigram probabilities with smoothing, adjusted for each word length group.\"\"\"\n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "    total_by_length = defaultdict(int)  # Total counts per word length\n",
    "    for word in words:\n",
    "        word_length = len(word)\n",
    "        for letter in word:\n",
    "            counts[word_length][letter] += 1\n",
    "            total_by_length[word_length] += 1\n",
    "    # Apply smoothing for each word length group\n",
    "    for word_length in counts:\n",
    "        for letter in alphabet:  # Ensure all possible letters are considered\n",
    "            counts[word_length][letter] = (counts[word_length][letter] + k) / (total_by_length[word_length] + k * 26)\n",
    "    return counts\n",
    "\n",
    "def calculate_bigram_probabilities(words):\n",
    "    \"\"\"Calculate bigram probabilities with starting symbol and smoothing.\"\"\"\n",
    "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    total_counts = defaultdict(int)\n",
    "    for word in words:\n",
    "        previous = '$'\n",
    "        for letter in word:\n",
    "            bigram_counts[previous][letter] += 1\n",
    "            total_counts[previous] += 1\n",
    "            previous = letter\n",
    "    # Apply smoothing\n",
    "    for previous in bigram_counts:\n",
    "        for letter in bigram_counts[previous]:\n",
    "            bigram_counts[previous][letter] = (bigram_counts[previous][letter] + k) / (total_counts[previous] + k*26)\n",
    "    return bigram_counts\n",
    "\n",
    "def build_ngram_model(words):\n",
    "    trie = Trie()\n",
    "    for word in words:\n",
    "        trie.insert(\"$\" + word) \n",
    "    trie.calculate_probabilities()\n",
    "    return trie\n",
    "\n",
    "def get_prefix_from_mask(mask):\n",
    "    ans = \"\"\n",
    "    for c in mask:\n",
    "        if c != \"_\":\n",
    "            ans += c\n",
    "        else:\n",
    "            break\n",
    "    return ans\n",
    "\n",
    "metrics = [0, 0, 0, 0]\n",
    "def my_amazing_ai_guesser(mask, guessed):\n",
    "    \"\"\"A guesser that uses a combination of bigram and unigram models.\"\"\"\n",
    "    # If this is the first guess, use the most common starting letter\n",
    "    sorted_unigram_counts = sorted(unigram_counts[len(mask)], key=unigram_counts[len(mask)].get, reverse=True)\n",
    "    # print(sorted_unigram_counts)\n",
    "    # if len(set(mask)) == 1: # haven't guessed a right letter\n",
    "    if not guessed:\n",
    "        for letter in sorted_unigram_counts:\n",
    "            # print(letter)\n",
    "            if letter not in guessed:\n",
    "                # print(\"Guessing start: \", letter)\n",
    "                return letter\n",
    "            else: \n",
    "                continue\n",
    "    \n",
    "    # print(mask, \": \", len(set(mask)) == 1, \"guessed: \", guessed)\n",
    "    # Calculate probabilities based on available ngrams\n",
    "    probabilities = defaultdict(float)\n",
    "    prev_empty = -1\n",
    "    word_length = len(mask) \n",
    "    # print(\"mask: \", mask)\n",
    "    for i, ch in enumerate(mask):\n",
    "        if ch == '_':\n",
    "            prefix = \"$\" if i == 0 else \"\" + \"\".join(mask[prev_empty+1:i]) \n",
    "            \n",
    "            prev_empty = i\n",
    "            # print(\"prefix: \", prefix)\n",
    "            # print(prefix)\n",
    "            # ngram_prob = ngram_model.find_one_letter_prob(prefix)\n",
    "            # for letter in ngram_prob:\n",
    "            #     if letter not in guessed:\n",
    "            #         metrics[0] += 1\n",
    "            #         probabilities[letter] += ngram_prob[letter] \n",
    "\n",
    "            previous = mask[i-1] if i > 0 else '$'\n",
    "            for letter in alphabet:\n",
    "                if letter not in guessed:\n",
    "                    probabilities[letter] += ngram_model.get_next_char_probability(prefix, letter, word_length) * 9\n",
    "                    metrics[0] += 1\n",
    "                    \n",
    "                    if previous in bigram_counts and letter in bigram_counts[previous]:\n",
    "                        probabilities[letter] += bigram_counts[previous][letter] + 1\n",
    "                        metrics[1] += 1\n",
    "                    \n",
    "                    \n",
    "                        # prediction = predict_next_letter(model, mask, \"\".join(guessed), verbose=0)\n",
    "                        # next_letter = get_most_probable_letter(prediction, \"\".join(guessed))\n",
    "                        # return next_letter\n",
    "                        # Fallback to unigram probabilities\n",
    "                    # else:\n",
    "                    probabilities[letter] += unigram_counts[len(mask)][letter] * 1\n",
    "                    metrics[2] += 1\n",
    "            \n",
    "    # if not probabilities:\n",
    "    #     # Fallback to random guess if no probabilities are calculated\n",
    "    #     metrics[3] += 1\n",
    "    #     letter = random.choice([ch for ch in alphabet if ch not in guessed])\n",
    "    #     # print(\"Guessing randomly: \", letter)\n",
    "    #     return letter\n",
    "    \n",
    "    # Pick the letter with the highest probability\n",
    "    letter = max(probabilities, key=probabilities.get)\n",
    "    # print(\"Guessing: \", letter)\n",
    "    return letter\n",
    "\n",
    "# Assume `training_set` is a list of words used for training the models\n",
    "unigram_counts = calculate_unigram_probabilities(training_set)\n",
    "bigram_counts = calculate_bigram_probabilities(training_set)\n",
    "ngram_model = build_ngram_model(training_set)\n",
    "ngram_counts = ngram_model.calculate_probabilities()\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "# small_test_set = [\"hello\"]\n",
    "# `test_guesser` needs to be defined to test the guesser function\n",
    "result = test_guesser(my_amazing_ai_guesser, training_set)\n",
    "print(\"Testing my amazing AI guesser using every word in test set\")\n",
    "print(\"Average number of incorrect guesses: \", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.17757009345794392, 'r': 0.11214953271028037, 'e': 0.09345794392523364, 'l': 0.08411214953271028, 'n': 0.08411214953271028, 's': 0.056074766355140186, 'd': 0.04672897196261682, 'i': 0.04672897196261682, 'm': 0.028037383177570093, 'u': 0.028037383177570093, 'w': 0.028037383177570093, 'f': 0.028037383177570093, 'y': 0.018691588785046728, 'g': 0.018691588785046728, 'c': 0.018691588785046728, 't': 0.018691588785046728, 'v': 0.018691588785046728, 'x': 0.018691588785046728}\n",
      "{'r': 0.18672199170124482, 's': 0.14927879865639201, 'd': 0.14522821576763487, 'n': 0.12052953961667655, 'l': 0.06181255351379833, 'a': 0.05153790423499967, 't': 0.04514918000395179, 'c': 0.03741026147665152, 'm': 0.030725153131792134, 'e': 0.025489033787788974, 'p': 0.018738062306527036, 'x': 0.01794770466969637, 'v': 0.016334057827833762, 'g': 0.013567806098926431, 'f': 0.013073832575907265, 'i': 0.012810380030297042, 'y': 0.01093328064282421, 'w': 0.009122044391753936, 'o': 0.007475465981690048, 'b': 0.006454587367450438, 'u': 0.006322861094645327, 'h': 0.004281103866166107, 'q': 0.00335901995653033, 'k': 0.0026345254561022198, 'z': 0.0019758940920766646, 'j': 0.0010867417506421656}\n",
      "{'e': 0.11044520547945205, 's': 0.09216357775987107, 'a': 0.09211321514907332, 'r': 0.06884568896051571, 'o': 0.06657937147461725, 'l': 0.06038477034649476, 'i': 0.058168815471394034, 't': 0.054844883158742946, 'n': 0.051974214343271555, 'd': 0.03862812248186946, 'c': 0.03500201450443191, 'u': 0.034045124899274776, 'h': 0.029814665592264304, 'm': 0.02830378726833199, 'y': 0.027296535052377117, 'p': 0.02628928283642224, 'b': 0.025282030620467365, 'g': 0.023015713134568894, 'k': 0.019087429492344883, 'f': 0.017475825946817083, 'w': 0.014957695406929896, 'v': 0.012036663980660757, 'z': 0.004381547139403707, 'j': 0.003978646253021757, 'x': 0.0033239323126510877, 'q': 0.0015612409347300564}\n"
     ]
    }
   ],
   "source": [
    "prefix = \"he\"\n",
    "ngram_prob = ngram_model.get_probability(prefix, 5)\n",
    "print({k: v for k, v in sorted(ngram_prob.items(), key=lambda item: item[1], reverse=True)})\n",
    "                    \n",
    "print({k: v for k, v in sorted(bigram_counts[\"e\"].items(), key=lambda item: item[1], reverse=True)})\n",
    "print({k: v for k, v in sorted(unigram_counts[5].items(), key=lambda item: item[1], reverse=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37494880, 17375140, 37494880, 0]\n",
      "0.40594295018995313\n"
     ]
    }
   ],
   "source": [
    "print(metrics)\n",
    "print(metrics[0] / sum(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average performance: 7.692328403960068\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# Seeds for different splits\n",
    "seeds = [0, 5, 10, 100, 200, 1000]\n",
    "performances = []\n",
    "\n",
    "for seed in seeds:\n",
    "    # Splitting the dataset into training and testing sets\n",
    "    train_set, test_set = train_test_split(unique_words, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Evaluating the guesser's performance on this split\n",
    "    performance = test_guesser(my_amazing_ai_guesser, test_set)\n",
    "    performances.append(performance)\n",
    "\n",
    "\n",
    "# Plotting overall performance across different seeds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Seed {}'.format(seed) for seed in seeds], performances)\n",
    "plt.xlabel('Seed')\n",
    "plt.ylabel('Performance')\n",
    "plt.title('Performance Across Different Seeds')\n",
    "plt.show()\n",
    "\n",
    "# Printing average performance\n",
    "print(\"Average performance:\", np.mean(performances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.692328403960068"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 (0.5 mark)\n",
    "\n",
    "**Instructions:** Explain your approach and discuss your result below. Please keep your explanation to a short paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Your answer BEGINS HERE\n",
    "\n",
    "- 2-gram 的命中率不足50%，所以要另外找到一个方法fall back\n",
    "\n",
    "\n",
    "##### Your answer ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
