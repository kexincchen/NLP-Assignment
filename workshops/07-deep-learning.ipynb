{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we will try to build some feedforward models to do sentiment analysis, using keras, a deep learning library: https://keras.io/\n",
    "\n",
    "You will need pandas, keras (2.10.0) and tensorflow (2.10.0; and their dependencies) to run this code (pip install pandas keras==2.10.0 tensorflow-cpu==2.10.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.4.1-cp39-cp39-macosx_10_9_x86_64.whl (11.5 MB)\n",
      "Collecting keras==2.3.1\n",
      "  Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
      "Collecting tensorflow-cpu==2.5.0\n",
      "  Downloading tensorflow_cpu-2.5.0-cp39-cp39-macosx_10_11_x86_64.whl (195.7 MB)\n",
      "     |████████████████████████████████| 195.7 MB 178 kB/s            \n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading h5py-3.6.0-cp39-cp39-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "     |████████████████████████████████| 3.1 MB 6.1 MB/s            \n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 2.1 MB/s            \n",
      "\u001b[?25hCollecting keras-applications>=1.0.6\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages (from keras==2.3.1) (1.22.3)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages (from keras==2.3.1) (1.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/Cellar/six/1.16.0_2/lib/python3.9/site-packages (from keras==2.3.1) (1.16.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/site-packages (from keras==2.3.1) (6.0)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.1.0-cp39-cp39-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "     |████████████████████████████████| 2.9 MB 6.0 MB/s            \n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     |████████████████████████████████| 65 kB 4.7 MB/s            \n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "     |████████████████████████████████| 132 kB 6.3 MB/s            \n",
      "\u001b[?25hCollecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp39-cp39-macosx_10_10_x86_64.whl (3.7 MB)\n",
      "     |████████████████████████████████| 3.7 MB 6.2 MB/s            \n",
      "\u001b[?25hCollecting six>=1.9.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.9/site-packages (from tensorflow-cpu==2.5.0) (0.37.1)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "     |████████████████████████████████| 462 kB 5.8 MB/s            \n",
      "\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 6.2 MB/s            \n",
      "\u001b[?25hCollecting numpy>=1.9.1\n",
      "  Downloading numpy-1.19.5-cp39-cp39-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "     |████████████████████████████████| 15.6 MB 6.4 MB/s            \n",
      "\u001b[?25hCollecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "     |████████████████████████████████| 5.8 MB 5.7 MB/s            \n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp39-cp39-macosx_10_9_x86_64.whl (961 kB)\n",
      "     |████████████████████████████████| 961 kB 6.1 MB/s            \n",
      "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "     |████████████████████████████████| 3.5 MB 6.1 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 6.4 MB/s            \n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.1.0-py3-none-any.whl (224 kB)\n",
      "     |████████████████████████████████| 224 kB 5.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow-cpu==2.5.0) (60.5.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 5.4 MB/s            \n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow-cpu==2.5.0) (2.27.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.2-py2.py3-none-any.whl (156 kB)\n",
      "     |████████████████████████████████| 156 kB 6.0 MB/s            \n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow-cpu==2.5.0) (0.2.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-cpu==2.5.0) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-cpu==2.5.0) (2021.10.8)\n",
      "Requirement already satisfied: charset_normalizer~=2.0.0 in /usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-cpu==2.5.0) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-cpu==2.5.0) (3.3)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow-cpu==2.5.0) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     |████████████████████████████████| 151 kB 5.9 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=09619815f22cefed7a0ff6fa4c27ba6a0cb9e3b14f4556b2e8ca81d1a2d8c064\n",
      "  Stored in directory: /Users/jeyhan/Library/Caches/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-macosx_11_0_x86_64.whl size=32860 sha256=98d9b81386187b09961b74f65a4739090a3ea41aa4bc15b8d6cd5587c2a671bf\n",
      "  Stored in directory: /Users/jeyhan/Library/Caches/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: zipp, six, rsa, oauthlib, cachetools, requests-oauthlib, numpy, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, h5py, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-nightly, keras-applications, google-pasta, gast, flatbuffers, astunparse, tensorflow-cpu, pandas, keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Not uninstalling six at /usr/local/Cellar/six/1.16.0_2/lib/python3.9/site-packages, outside environment /usr/local/Cellar/jupyterlab/3.2.9/libexec\n",
      "    Can't uninstall 'six'. No files were found to uninstall.\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.3\n",
      "    Uninstalling numpy-1.22.3:\n",
      "      Successfully uninstalled numpy-1.22.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.0.1\n",
      "    Not uninstalling typing-extensions at /usr/local/lib/python3.9/site-packages, outside environment /usr/local/Cellar/jupyterlab/3.2.9/libexec\n",
      "    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "black 22.1.0 requires typing-extensions>=3.10.0.0; python_version < \"3.10\", but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.15.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.6.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 importlib-metadata-4.11.3 keras-2.3.1 keras-applications-1.0.8 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.6 numpy-1.19.5 oauthlib-3.2.0 opt-einsum-3.3.0 pandas-1.4.1 protobuf-3.19.4 requests-oauthlib-1.3.1 rsa-4.8 six-1.15.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-cpu-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 werkzeug-2.1.0 wrapt-1.12.1 zipp-3.7.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.2.9/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas keras==2.10.0 tensorflow-cpu==2.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's prepare the data. We are using 1000 yelp reviews, nnotated with either positive or negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences = 1000\n",
      "\n",
      "Data:\n",
      "                                    sentence  label\n",
      "0                   Wow... Loved this place.      1\n",
      "1                         Crust is not good.      0\n",
      "2  Not tasty and the texture was just nasty.      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus = \"07-yelp-dataset.txt\"\n",
    "df = pd.read_csv(corpus, names=['sentence', 'label'], sep='\\t')\n",
    "print(\"Number of sentences =\", len(df))\n",
    "print(\"\\nData:\")\n",
    "print(df.iloc[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the train/dev/test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Wow... Loved this place.\n",
      "0 I'm super pissd.\n",
      "0 Spend your money elsewhere.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "sentences = df['sentence'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "#partition data into 80/10/10 for train/dev/test\n",
    "sentences_train, y_train = sentences[:800], labels[:800]\n",
    "sentences_dev, y_dev = sentences[800:900], labels[800:900]\n",
    "sentences_test, y_test = sentences[900:1000], labels[900:1000]\n",
    "\n",
    "#convert label list into arrays\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(y_train[0], sentences_train[0])\n",
    "print(y_dev[0], sentences_dev[0])\n",
    "print(y_test[0], sentences_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the text. In this workshop, we'll use the ``tokenizer`` function provided by keras. Once the data is tokenized, we can then use ``texts_to_matrix`` to get the bag-of-words representation for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(sentences_train)\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages/keras/__init__.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages/keras/utils/__init__.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_source_inputs\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_summary\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvis_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_to_dot\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvis_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_model\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages/keras/utils/vis_utils.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Wrapper\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# `pydot` is an optional dependency,\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# see `extras_require` in `setup.py`.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages/keras/models.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_arg\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_list\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputLayer\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages/keras/engine/__init__.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# note: `Node` is an internal class,\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# it isn't meant to be used by Keras users.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputLayer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputSpec\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages/keras/engine/input_layer.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Layer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Node\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages/keras/engine/base_layer.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m count_params\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_arg\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages/keras/initializers/__init__.py:124\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m   LOCAL\u001b[38;5;241m.\u001b[39mALL_OBJECTS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzero\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m LOCAL\u001b[38;5;241m.\u001b[39mALL_OBJECTS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# For backwards compatibility, we populate this file with the objects\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# from ALL_OBJECTS. We make no guarantees as to whether these objects will\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# using their correct version.\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[43mpopulate_deserializable_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(LOCAL\u001b[38;5;241m.\u001b[39mALL_OBJECTS)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Utility functions\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages/keras/initializers/__init__.py:82\u001b[0m, in \u001b[0;36mpopulate_deserializable_objects\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m v2_objs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     81\u001b[0m base_cls \u001b[38;5;241m=\u001b[39m initializers_v2\u001b[38;5;241m.\u001b[39mInitializer\n\u001b[0;32m---> 82\u001b[0m \u001b[43mgeneric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulate_dict_with_module_objects\u001b[49m(\n\u001b[1;32m     83\u001b[0m     v2_objs,\n\u001b[1;32m     84\u001b[0m     [initializers_v2],\n\u001b[1;32m     85\u001b[0m     obj_filter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: inspect\u001b[38;5;241m.\u001b[39misclass(x) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x, base_cls))\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m v2_objs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     87\u001b[0m   LOCAL\u001b[38;5;241m.\u001b[39mALL_OBJECTS[key] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(sentences_train, mode=\"count\") #BOW representation\n",
    "x_dev = tokenizer.texts_to_matrix(sentences_dev, mode=\"count\") #BOW representation\n",
    "x_test = tokenizer.texts_to_matrix(sentences_test, mode=\"count\") #BOW representation\n",
    "\n",
    "vocab_size = x_train.shape[1]\n",
    "print(\"Vocab size =\", vocab_size)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we build a neural network model, let's see how well logistic regression do with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laujh/.pyenv/versions/3.6.9/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "score = classifier.score(x_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression result is not too bad, and it will serve as a baseline for the deep learning models.\n",
    "\n",
    "Now let's build a very simple feedforward network. Here the input layer is the BOW features, and we have one hidden layer (dimension = 10) and an output layer in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feedforward-bow-input\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                18120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 18,131\n",
      "Trainable params: 18,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "#model definition\n",
    "model = Sequential(name=\"feedforward-bow-input\")\n",
    "model.add(layers.Dense(10, input_dim=vocab_size, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#since it's a binary classification problem, we use a binary cross entropy loss here\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model. Notice that there are a few hyper-parameters here, e.g. hidden layer size, number of epochs and batch_size, and in practice these hyper-parameters should be tuned according to the development data to get an optimal model. In this workshop we'll use 20 epochs and a batch size of 10 (no further tuning). Once the model is trained, we'll compute the test accuracy performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 0s 399us/step - loss: 0.6876 - accuracy: 0.5475 - val_loss: 0.6790 - val_accuracy: 0.6300\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.6340 - accuracy: 0.7775 - val_loss: 0.6304 - val_accuracy: 0.7000\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.5328 - accuracy: 0.8425 - val_loss: 0.5619 - val_accuracy: 0.7700\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.4159 - accuracy: 0.9100 - val_loss: 0.5081 - val_accuracy: 0.7800\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 0s 254us/step - loss: 0.3230 - accuracy: 0.9500 - val_loss: 0.4694 - val_accuracy: 0.8100\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.2528 - accuracy: 0.9613 - val_loss: 0.4428 - val_accuracy: 0.8200\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 0s 207us/step - loss: 0.2003 - accuracy: 0.9750 - val_loss: 0.4162 - val_accuracy: 0.8300\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 0s 166us/step - loss: 0.1614 - accuracy: 0.9837 - val_loss: 0.4057 - val_accuracy: 0.8300\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1315 - accuracy: 0.9887 - val_loss: 0.3980 - val_accuracy: 0.8100\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1089 - accuracy: 0.9912 - val_loss: 0.3968 - val_accuracy: 0.8100\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.0913 - accuracy: 0.9937 - val_loss: 0.3848 - val_accuracy: 0.8100\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0773 - accuracy: 0.9950 - val_loss: 0.3913 - val_accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.0656 - accuracy: 0.9975 - val_loss: 0.3939 - val_accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.0565 - accuracy: 0.9987 - val_loss: 0.3814 - val_accuracy: 0.8000\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0494 - accuracy: 0.9987 - val_loss: 0.3860 - val_accuracy: 0.7900\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 0s 193us/step - loss: 0.0431 - accuracy: 0.9987 - val_loss: 0.4015 - val_accuracy: 0.8100\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 0s 189us/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.4027 - val_accuracy: 0.8100\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.4063 - val_accuracy: 0.8100\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.4202 - val_accuracy: 0.8100\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.4196 - val_accuracy: 0.8100\n",
      "\n",
      "Testing Accuracy:  0.7800\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "model.fit(x_train, y_train, epochs=20, verbose=True, validation_data=(x_dev, y_dev), batch_size=10)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(\"\\nTesting Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the performance compare to logistic regression? If you run it a few times you may find that it gives slightly different numbers, and that is due to random initialisation of the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we did not explicitly define any word embeddings in the model architecture, they are in our model: in the weights between the input and the hidden layer. The hidden layer can therefore be interpreted as a sum of word embeddings for each input document.\n",
    "\n",
    "Let's fetch the word embeddings of some words, and look at their cosine similarity, and see if they make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.21396367  0.27749586  0.2945718  -0.13073313 -0.22764814 -0.2310485\n",
      "  0.33856174 -0.20201477 -0.2462762   0.23290806]\n",
      "love vs. like = 0.8865069\n",
      "love vs. lukewarm = -0.966434\n",
      "love vs. bad = -0.97216696\n",
      "lukewarm vs. bad = 0.97636676\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "embeddings = model.get_layer(index=0).get_weights()[0] #word embeddings layer\n",
    "\n",
    "emb_love = embeddings[tokenizer.word_index[\"love\"]] #embeddings for 'love'\n",
    "emb_like = embeddings[tokenizer.word_index[\"like\"]]\n",
    "emb_lukewarm = embeddings[tokenizer.word_index[\"lukewarm\"]]\n",
    "emb_bad = embeddings[tokenizer.word_index[\"bad\"]]\n",
    "\n",
    "print(emb_love)\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "print(\"love vs. like =\", cos_sim(emb_love, emb_like))\n",
    "print(\"love vs. lukewarm =\", cos_sim(emb_love, emb_lukewarm))\n",
    "print(\"love vs. bad =\", cos_sim(emb_love, emb_bad))\n",
    "print(\"lukewarm vs. bad =\", cos_sim(emb_lukewarm, emb_bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. You should find that for *love* and *like*, which are both positive sentiment words, produce high cosine similarity. Similar observations for *lukewarm* and *bad*. But when we compare opposite polarity words like *love* and *bad*, we get negative cosine similarity values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to build another feed-forward model, but this time, instead of using BOW features as input, we want to use the word sequence as input (so order of words is preserved). It is usually not straightforward to do this for classical machine learning models, but with neural networks and embeddings, it's pretty straightforward.\n",
    "\n",
    "Let's first tokenise the input documents into word sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[354, 138, 9, 17]\n"
     ]
    }
   ],
   "source": [
    "#tokenise the input into word sequences\n",
    "\n",
    "xseq_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "xseq_dev = tokenizer.texts_to_sequences(sentences_dev)\n",
    "xseq_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "print(xseq_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because documents have variable lengths, we need to first 'pad' them to make all documents have the same length. keras uses word index 0 to represent 'pad symbols'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[354 138   9  17   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 30\n",
    "xseq_train = pad_sequences(xseq_train, padding='post', maxlen=maxlen)\n",
    "xseq_dev = pad_sequences(xseq_dev, padding='post', maxlen=maxlen)\n",
    "xseq_test = pad_sequences(xseq_test, padding='post', maxlen=maxlen)\n",
    "print(xseq_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build our second model. This model first embeds each word in the input sequence into embeddings, and then concatenate the word embeddings together to represent input sequence. The ``Flatten`` function you see after the embedding layer is essentially doing the concatenation, by 'chaining' the list of word embeddings into a very long vector.\n",
    "\n",
    "If our word embeddings has a dimension 10, and our documents always have 30 words (padded), then here the concatenated word embeddings have a dimension of 10 x 30 = 300. \n",
    "\n",
    "The concatenated word embeddings undergo a linear transformation with non-linear activations (``layers.Dense(10, activation='relu')``), producing a hidden representation with a dimension of 10. It is then passed to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feedforward-sequence-input\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 10)            18110     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                3010      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 21,131\n",
      "Trainable params: 21,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10\n",
    "\n",
    "#word order preserved with this architecture\n",
    "model2 = Sequential(name=\"feedforward-sequence-input\")\n",
    "model2.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(10, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model and compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laujh/.pyenv/versions/3.6.9/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 0s 321us/step - loss: 0.6925 - accuracy: 0.5462 - val_loss: 0.6948 - val_accuracy: 0.4400\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.6828 - accuracy: 0.7175 - val_loss: 0.6946 - val_accuracy: 0.4400\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.6560 - accuracy: 0.7675 - val_loss: 0.6851 - val_accuracy: 0.6200\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 0s 127us/step - loss: 0.5742 - accuracy: 0.8375 - val_loss: 0.6498 - val_accuracy: 0.6300\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.4661 - accuracy: 0.9262 - val_loss: 0.6240 - val_accuracy: 0.6800\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.3898 - accuracy: 0.9663 - val_loss: 0.6079 - val_accuracy: 0.7300\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.3401 - accuracy: 0.9887 - val_loss: 0.6138 - val_accuracy: 0.7100\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.3047 - accuracy: 0.9975 - val_loss: 0.6036 - val_accuracy: 0.7200\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.2775 - accuracy: 0.9987 - val_loss: 0.6233 - val_accuracy: 0.7000\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.2565 - accuracy: 1.0000 - val_loss: 0.6262 - val_accuracy: 0.7100\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.2380 - accuracy: 1.0000 - val_loss: 0.6243 - val_accuracy: 0.7200\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 0s 171us/step - loss: 0.2229 - accuracy: 1.0000 - val_loss: 0.6462 - val_accuracy: 0.7200\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.2091 - accuracy: 1.0000 - val_loss: 0.6473 - val_accuracy: 0.7200\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.1967 - accuracy: 1.0000 - val_loss: 0.6571 - val_accuracy: 0.7100\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1855 - accuracy: 1.0000 - val_loss: 0.6629 - val_accuracy: 0.7000\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1752 - accuracy: 1.0000 - val_loss: 0.6687 - val_accuracy: 0.7100\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1658 - accuracy: 1.0000 - val_loss: 0.6733 - val_accuracy: 0.7100\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1570 - accuracy: 1.0000 - val_loss: 0.6704 - val_accuracy: 0.7100\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 0s 160us/step - loss: 0.1490 - accuracy: 1.0000 - val_loss: 0.6753 - val_accuracy: 0.7100\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 0s 164us/step - loss: 0.1414 - accuracy: 1.0000 - val_loss: 0.6817 - val_accuracy: 0.7100\n",
      "Testing Accuracy:  0.8100\n"
     ]
    }
   ],
   "source": [
    "model2.fit(xseq_train, y_train, epochs=20, verbose=True, validation_data=(xseq_dev, y_dev), batch_size=10)\n",
    "\n",
    "loss, accuracy = model2.evaluate(xseq_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find that the performance isn't as good as the BOW model. In general, concatenating word embeddings isn't a good way to represent word sequence.\n",
    "\n",
    "A better way is to build a recurrent model. But first, let's extract the word embeddings for the 4 words as before and look at their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love vs. like = -0.29512215\n",
      "love vs. lukewarm = -0.10457008\n",
      "love vs. bad = -0.63287705\n",
      "lukewarm vs. bad = 0.1924367\n"
     ]
    }
   ],
   "source": [
    "embeddings = model2.get_layer(index=0).get_weights()[0] #word embeddings\n",
    "\n",
    "emb_love = embeddings[tokenizer.word_index[\"love\"]]\n",
    "emb_like = embeddings[tokenizer.word_index[\"like\"]]\n",
    "emb_lukewarm = embeddings[tokenizer.word_index[\"lukewarm\"]]\n",
    "emb_bad = embeddings[tokenizer.word_index[\"bad\"]]\n",
    "\n",
    "print(\"love vs. like =\", cos_sim(emb_love, emb_like))\n",
    "print(\"love vs. lukewarm =\", cos_sim(emb_love, emb_lukewarm))\n",
    "print(\"love vs. bad =\", cos_sim(emb_love, emb_bad))\n",
    "print(\"lukewarm vs. bad =\", cos_sim(emb_lukewarm, emb_bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to build an LSTM model. After the embeddings layer, the LSTM layer will process the words one at a time, and compute the next state (dimension for the hidden state = 10 in this case). The output of the LSTM layer is the final state, produced after processing the last word, and that will be fed to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 10)            18110     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 18,961\n",
      "Trainable params: 18,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "#word order preserved with this architecture\n",
    "model3 = Sequential(name=\"lstm\")\n",
    "model3.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model3.add(LSTM(10))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the LSTM model and see the test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laujh/.pyenv/versions/3.6.9/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.6893 - accuracy: 0.5650 - val_loss: 0.7060 - val_accuracy: 0.4400\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.6838 - accuracy: 0.5650 - val_loss: 0.7292 - val_accuracy: 0.4400\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.6749 - accuracy: 0.5675 - val_loss: 0.7452 - val_accuracy: 0.4400\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4584 - accuracy: 0.7975 - val_loss: 0.5791 - val_accuracy: 0.7300\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2807 - accuracy: 0.9175 - val_loss: 0.6209 - val_accuracy: 0.7600\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1843 - accuracy: 0.9525 - val_loss: 0.4598 - val_accuracy: 0.7800\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1304 - accuracy: 0.9700 - val_loss: 0.4873 - val_accuracy: 0.8000\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0824 - accuracy: 0.9837 - val_loss: 0.5476 - val_accuracy: 0.8000\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0537 - accuracy: 0.9925 - val_loss: 0.8906 - val_accuracy: 0.7500\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0501 - accuracy: 0.9912 - val_loss: 0.7342 - val_accuracy: 0.7900\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0317 - accuracy: 0.9962 - val_loss: 0.6869 - val_accuracy: 0.8300\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0282 - accuracy: 0.9962 - val_loss: 0.8365 - val_accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0263 - accuracy: 0.9962 - val_loss: 0.8690 - val_accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0253 - accuracy: 0.9962 - val_loss: 0.9037 - val_accuracy: 0.8000\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0246 - accuracy: 0.9962 - val_loss: 0.9340 - val_accuracy: 0.8000\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0241 - accuracy: 0.9962 - val_loss: 0.9678 - val_accuracy: 0.8000\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0234 - accuracy: 0.9962 - val_loss: 1.0190 - val_accuracy: 0.8000\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0342 - accuracy: 0.9950 - val_loss: 1.0346 - val_accuracy: 0.7700\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0314 - accuracy: 0.9925 - val_loss: 1.1278 - val_accuracy: 0.7700\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0442 - accuracy: 0.9900 - val_loss: 0.8666 - val_accuracy: 0.7900\n",
      "Testing Accuracy:  0.7700\n"
     ]
    }
   ],
   "source": [
    "model3.fit(xseq_train, y_train, epochs=20, verbose=True, validation_data=(xseq_dev, y_dev), batch_size=10)\n",
    "\n",
    "loss, accuracy = model3.evaluate(xseq_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the training is quite a bit slower, and that's because now the model has to process the sequence one word at a time. But the results should be better!\n",
    "\n",
    "And lastly, let's extract the embeddings and look at the their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love vs. like = 0.42321843\n",
      "love vs. lukewarm = -0.95175457\n",
      "love vs. bad = -0.98218477\n",
      "lukewarm vs. bad = 0.93053085\n"
     ]
    }
   ],
   "source": [
    "embeddings = model3.get_layer(index=0).get_weights()[0] #word embeddings\n",
    "\n",
    "emb_love = embeddings[tokenizer.word_index[\"love\"]]\n",
    "emb_like = embeddings[tokenizer.word_index[\"like\"]]\n",
    "emb_lukewarm = embeddings[tokenizer.word_index[\"lukewarm\"]]\n",
    "emb_bad = embeddings[tokenizer.word_index[\"bad\"]]\n",
    "\n",
    "print(\"love vs. like =\", cos_sim(emb_love, emb_like))\n",
    "print(\"love vs. lukewarm =\", cos_sim(emb_love, emb_lukewarm))\n",
    "print(\"love vs. bad =\", cos_sim(emb_love, emb_bad))\n",
    "print(\"lukewarm vs. bad =\", cos_sim(emb_lukewarm, emb_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
